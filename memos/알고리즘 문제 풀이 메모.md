## 파이썬 알고리즘 문제 풀이 메모

### Greedy

### Implementation

---

### DFS / BFS

#### Stack and Queue

##### Stack

​	파이썬에서 스택은 list로 해결 가능함.

##### Queue

​	deque 사용.

```python
from collections import deque

q = deque()

# append
q.append(5)
# pop q
q.popleft()
# reversing q
q.reverse()

# make queue to list
q = list(q)
```

#### 재귀함수

​	종료 조건 명시

​	점화식

#### Graph

​	Node (Vertex) - 정점

​	Edge - Node 사이 연결하는 선

​	인접하다 (Adjacent, adj) - Node 사이에 Edge가 존재한다.

##### 표현 방법

​	python에서는 2차원 list로 모두 구현 가능 (C++, Java는 Linked List는 표준 라이브러리로 제공)

  - 인접 행렬 - 2차원 array
    - 메모리 낭비가 있음
    - 정보 얻는 속도가 빠름 (배열 원소만 확인하면 됨)
  - 인접 리스트 - linked list
    - 메모리 낭비가 적음
    - 정보 얻는 속도가 느림 (데이터 하나씩 확인이 필요)
    - 인접 노드 전체 순환이 필요할 때 유리

#### DFS

​	Depth First Search (깊이 우선 탐색)

​	Stack

​	O(N)

##### 동작 과정

1. Root 노드 스택 삽입 및 방문 처리

2.  스택의 Top 노드의 "방문하지 않은" 인접 노드가 있으면 해당 노드를 Push 및 방문 처리, 없으면 Top 노드를 Pop
1.  더 이상 수행할 수 없을 때 까지 반복 

​	일반적으로 낮은 번호 순으로 처리 (방문)

##### 예제

```python
def dfs(graph, v, visited):
	# 현재 노드 방문 처리
    visited[v] = True
	print(v, end=' ')
    # 인접 노드 재귀적으로 방문
    for i in graph[v]:
        if not visited[i]:
            dfs(graph, i, visited)
            
graph = [
    [],
    [2,3],
    [1,4,5],
    [1,6,7],
    [],
    [],
    [],
    []
]

visited = [False] * NUMBER_OF_NODES

# 호출
dfs(graph, 1, visited)
```

#### BFS

​	Breadth First Search (너비 우선 탐색)

​	Queue

​	O(N)

​	일반적으로 DFS보다 수행 시간이 좋은 편

##### 동작 과정

 1.  Root 노드 Queue 삽입 및 방문 처리

 2.  Queue에서 노드 Pop

     해당 노드의 "인접한" 노드 중 "방문하지 않은" 노드를 모두 Queue에 넣고 방문 처리

	더 이상 수행할 수 없을 때까지 반복



##### 예제

```python
from collections import deque

def bfs(graph, start, visited):
    # root node 방문 처리
    q = deque([start])
    visited[start] = True
    
    while q:
        v = q.popleft()
        print(v, end=' ')
        for i in graph[v]:
            # 인접 노드 중 미 방문 노드
            if not visited[v]:
                q.append(i)
                visited[i] = True

graph = [
    [],
    [2,3],
    [1,4,5],
    [1,6,7],
    [],
    [],
    [],
    []
]

visited = [False] * NUMBER_OF_NODES

bfs(graph, 1, visited)
```

---

### Sorting

​	오름차순 기준

​	내림차순은 오름차순 정렬 후 reverse. (O(N) 시간 안에 해결 가능)

#### Selection Sort

​	선택 정렬

##### 개요

​	가장 작은 데이터부터 맨 앞 데이터와 바꾸는 작업을 반복 수행

###### 시간 복잡도

​	O(N^2^)

##### 예제

```python
# Selection Sort

for i in range(len(array)):
    min_index = i
    for j in range(i+1, len(array)):
        if array[min_index] > array[j]:
            min_index = j
    # swap
    array[i], array[min_index] = array[min_index], array[i]
```

###### 참고

​	swap in python

```python
a, b = b, a
```



#### Insertion Sort

​	삽입 정렬

##### 개요

​	값 하나씩 확인해서 적절한 위치에 삽입

​	항상 오름차순으로 정렬됨

​	거의 정렬된 리스트에 대해서 매우 효율적 (최선의 경우 O(N))

###### 시간 복잡도

​	평균: O(N^2^)
​	최선: O(N)

##### 예제

```python
# Insertion Sort
for i in range(1, len(array)):
    # range(start, end, step)
    for j in range (i, 0, -1):
        if array[j] < array[j-1]:
            array[j], array[j-1] = array[j-1], array[j]
        # 작은 값 나올 때 까지 이동
        else:
            break
```



#### Quick Sort

​	퀵 정렬

##### 개요

​	피벗 기준 큰 데이터와 작은 데이터로 나눠서 정렬

​	여러 분할 방법이 있음 (피벗 설정)

​	Hoare Partition - 첫 번째 데이터를 피벗으로 정함

​	이미 데이터가 정렬되어 있을 경우 느리게 동작

###### 시간 복잡도

​	평균: O(NlogN)
​	최악: O(N^2^)

###### 정렬 과정

​	divide - conquer - combine

##### 예제

###### 정석 코드

  ```python
  # Quick Sort - 1
  def quick_sort(array, start, end):
      if start >= end:
          return
      pivot = start
      left = start+1
      right = end
      while left <= right:
          while left <= end and array[left] <= array[pivot]:
              left += 1
          while right > start and array[rigght] >= array[pivot]:
              right -= 1
          if left > right:
              array[right], array[pivot] = array[pivot], array[right]
          else:
              array[left], array[right] = array[right], array[left]
      
      # divide and conquer
      quick_sort(array, start, right-1)
      quick_sort(array, right+1, end)
  
  quick_sort(array, 0, len(array)-1)
  ```

######   직관적인 코드

​	연산 횟수가 증가하므로 시간 면에서 조금 비효율적

```python
# Quick Sort - 2
def quick_sort(array):
    if len(array) <= 1:
        return array
    
    pivot = array[0]
    tail = array[1:]
    
    left = [x for x in tail if x <= pivot]
    right = [x for x in tail if x > pivot]
    
    return quick_sort(left) + [pivot] + quick_sort(right)
```



#### Count Sort

​	계수 정렬

##### 개요

​	특정 조건 부합할 경우 매우 빠름
​	(데이터 크기 범위 제한 및 정수로 표현할 수 있을 때)

​	일반적으로 100만개 이하일 때 사용 가능

###### 시간 복잡도

​	O(N+K) (K: max value of data)

##### 예제

```python
# Count Sort
count = [0] * (max(array)+1)

for i in range(len(array)):
    count[array[i]] += 1
    
for i in range(len(count)):
    for j in range(count[i]):
        print(i, end=' ')
```



#### Python Sort Library

​	파이썬 내부 정렬 함수, 리스트 메서드

##### 개요

​	Merge Sort 기반

​	list, set, dictionary 입력 가능

​	list로 반환

###### 시간 복잡도

​	O(NlogN)

##### 예제

```python
# sorted: 정렬된 list 반환
result = sorted(array)

# list 내부 원소 정렬
array.sort()

# key 사용
array = [(1,'a'), (2,'b'), (3,'c')]

# without lambda
def setting(data):
    return data[0]

result = sorted(array, key=setting)

# with lambda
result = sorted(array, key=lambda data: data[0])
```



#### 코테에서의 정렬 알고리즘

1. Library 사용
2. 정렬 알고리즘 원리
3. complicated sort: 더 빠른 정렬 알고리즘 사용 혹은 알고리즘 개선으로 문제 풀이

---

### Binary Search

#### Sequential Search

​	순차 탐색

##### 개요

​	앞에서부터 데이터를 하나씩 찾는 방법

​	count() 메서드 등에도 사용됨

###### 시간 복잡도

​	O(N)

###### 예제

```python
# Sequential Search
def sequential_search(n, target, array):
    for i in range(n):
        if array[i] == target:
            return i
```



#### Binary Search

​	이진 탐색

##### 개요

​	배열 내부가 정렬 되어 있어야만 사용 가능

​	범위를 절반씩 좁혀가며 탐색

​	평균적으로 한 단계마다 원소 수가 절반으로 줄어듦 (log_2_N == O(logN))

###### 원리

​	위치 변수 3개 사용 (시작점, 끝점, 중간점)

​	Target과 Middle값을 반복적으로 비교해서 데이터 탐색

###### 시간 복잡도

​	O(logN) - 한번 탐색할 때 마다 절반씩 줄어듦 (퀵 소트와 비슷)

##### 예제

###### 재귀 함수

```python
def binary_search(array, target, start, end):
    if start > end:
        return None
    
    mid = (start + end) // 2
    # mid = int((start+end)/2)
    
    if (array[mid] == target):
        return mid
    # 찾는 값이 더 크면 (오른쪽)
    elif (array[mid] < target):
        binary_search(array, target, mid + 1, end)
    # 찾는 값이 더 작으면 (왼쪽)
    else:
        binary_search(array, target, start, mid - 1)
```



###### 반복문

```python
def binary_search(array, target, start, end):
    while start <= end:
        mid = (start + end) // 2
        
        if (array[mid] == target):
            return mid
        # 찾는 값이 더 크면 (오른쪽)
        elif (array[mid] < target):
            start = mid + 1
        # 찾는 값이 더 작으면 (왼쪽)
        else:
            end = mid - 1
        
        return None
```



##### 코테에서의 이진 탐색

​	탐색 범위가 큰 상황 (2000만 이상 등)

​	1000만 이상이면 O(logN)으로 접근

​	그리디와 함께 출제되는 형태가 있음



#### BST

​	Binary Search Tree

##### 개요

​	왼쪽 자식노드 < 부모 노드 < 오른쪽 자식 노드



##### 참고

###### Parametric Search

​	최적화 문제를 결정 문제 (yes or no)로 바꾸어 해결하는 기법

​	조건을 만족하는 가장 알맞은 값을 찾는 문제에 주로 사용 (가장 큰 값, 가장 작은 값 등의 최적화 문제)

​	예제 3번의 경우에는 H를 조정해서 만족하는지 여부를 체크하면 됨.

---

#### Dynamic Programming

##### 개요

​	큰 문제를 작게 나누고 같은 문제라면 한 번씩만 풀어서 문제를 해결하는 방법

​	Divide and Conquer와의 차이점: DP는 분할된 문제들이 서로 영향을 미친다



##### DP 사용 조건

1. 큰 문제를 작은 문제로 나눌 수 있다
2. 작은 문제의 정답은 큰 문제에서도 동일하다.



##### 메모이제이션 (캐싱)

​	한 번 구한 값을 메모리에 저장해 두고 다시 호출하면 값을 그대로 가져오는 기법

​	계산된 값을 리스트에 저장하는 방법으로 구현 가능



##### Top-Down

​	재귀 함수

​	큰 문제를 해결하기 위해 작은 문제를 호출

​	하향식

​	메모이제이션



###### 예제

```python
# fibonacci with dp (top-down)

d = [0]*100

def fibo(x):
    if x == 1 or x == 2:
        return x
    
    # 이미 계산한 값이면 그 값 리턴
    if d[x] != 0:
        return d[x]
    
    # 값 계산 후 저장(메모)
    d[x] = fibo(x-1) + fibo(x-2)
    
    return d[x]
```

시간복잡도: O(N)

=> 한 번 구한 결과는 더 이상 구할 필요가 없으므로



##### Bottom up

​	반복문

​	작은 문제부터 답 도출

​	상향식

​	DP의 전형적인 형태

​	DP 테이블 (데이터 저장 리스트)



###### 예제

```python
# fibonacci with dp (bottom-up)
d = [0] * 100

d[1] = 1
d[2] = 1
n = 99

for i in range(3, n+1):
    d[i] = d[i-2]+d[i-1]
```



##### 참고

​	메모이제이션 != 다이나믹 프로그래밍

​	수열 => 배열, 리스트, 딕셔너리(일부 값만 필요할 때) 사용 가능



###### 문제 풀이 요령

​	유형 파악이 중요

 	1. 완전탐색이 매우 오래 걸릴 때
 	 - 부분 문제  중복 여부 확인
 	2. 재귀 함수로 일단 풀어보기 (Top Down)
 	 - 메모이제이션 적용 가능하면 코드 개선
 	3. Bottom up으로 구현을 권장
 	 - 재귀함수 스택이 한정 되어 있음 (recursion depth error)
 	 - sys.setrecursionlimit()으로 완화 가능

---

#### Shortest Path

​	가장 짧은 경로 찾기 (길 찾기)

​	Greedy, Dynamic Programming 적용



##### Graph

​	지점: Node, 간선: Edge

​	노드 사이의 최단 거리 알고리즘

​	Dijkstra, Floyd, Bellman-Ford



##### Djikstra

​	최단 경로 알고리즘

​	특정 노드에서 다른 노드로 가는 각 최단 경로
​	(= 한 지점에서 다른 모든 지점 까지의 최단 경로)

​	음의 간선이 없어야 정상적으로 동작함

​	Greedy 알고리즘 (가장 비용 적은 노드 선택)



###### 과정

```
1. 시작 노드 설정
2. 최단 거리 테이블 큰 값으로 초기화 (int(ie9))
3. (간선으로 연결된) 미방문 노드 중 최단 거리가 가장 짧은 노드를 선택 (최단 테이블에서 가장 작은 값)
4. 선택한 노드를 거쳐 다른 노드로 가능 비용 계산해서 최단거리 테이블 갱신
5. 3, 4 반복
```

​	각 노드에 대해 최단 거리를 계속 갱신

​	현재 노드 기준으로 짧은 경로 판단

​	한 단계당 하나의 노드에 대한 최단 거리를 확실히 찾을 수 있음
​	=> 3번 단계: 최단 거리 찾기

###### 구현

1. O(V^2^) 알고리즘 (V = num of nodes)

   adj list 사용

   단계마다 가장 짧은 노드를 선택하기 위해 리스트의 모든 원소를 `순차 탐색`함

   ```python
   import sys
   input = sys.stdin.readline
   INF = int(1e9) 
   # INF = 987654321
   
   # node, edge
   n, e = map(int, input().split())
   
   start = int(input())
   
   graph = [[] for i in range(n+1)]
   
   visited = [False] * (n + 1)
   
   distance = [INF] * (n + 1)
   
   for _ in range(e):
       a, b, weight = map(int, input().split())
       graph[a].append((b,weight))
       
   # 미방문 노드 중 가장 최단거리가 짧은 노드 찾기
   def get_smallest_node():
       min_value = INF
       index = 0
       for i in range(1,n+1):
           if distance[i] < min_value and not visited[i]:
               min_value = distance[i]
               index = i            
   	return index
   
   def dijkstra(start):
       # init
       distance[start] = 0
       visited[start] = True
       for j in graph[start]:
           distance[j[0]] = j[1]
       # n-1번 반복
       for i in range(n-1):
           # 현재 노드 찾고 방문처리
           cur = get_smallest_node()
           visited[cur] = True
           for j in graph[cur]:
               cost = distance[cur] + j[1]
               # 최단거리 비교 후 갱신
               if cost < distance[j[0]]:
                   distance[j[0]] = cost
   ```

   시간 복잡도: O(V^2^): V번 선형 탐색

   

2. O(ElogV) 알고리즘 (E = num of edges)

   가장 가까운 노드 저장하기 위해서 Min Heap(pq) 사용

   pq에서 노드를 꺼냈을 때 이미 처리했으면 (방문했으면)  해당 노드는 무시해도 됨
   
   ```python
   import heapq
   import sys
   input = sys.stdin.readline
   INF = int(ie9)
   
   n, e = map(int, input().split())
   start = int(input())
   graph = [[] for i in range(n+1)]
   distance = [INF] * (n+1)
   
   for _ in range(e):
       a, b, weight = map(int, input().split())
       graph[a].append((b,weight))
   
   def dijkstra(start):
       q = []
       # start node push
       heapq.heappush(q,(0,start))
       distance[start] = 0
       while q:
           # 최단 거리 노드 꺼내기
           dist, cur = heapq.heappop(q)
           # 이미 방문한 노드면 무시
           if distance[cur] < dist:
               continue
           for i in graph[cur]:
               cost = dist + i[1]
               if cost < distance[i[0]]:
                   distance[i[0]] = cost
                   # 거리, 노드 순으로 push
                   heapq.heappush(q, (cost,i[0]))
   ```
   
   시간 복잡도: O(VlogE)
   
   - heapq 단일 데이터 삽입/삭제: O(logN)
   



##### Floyd-Warshall

​	각 지점에서 다른 모든 지점까지의 최단 경로

​	방문 노드별 최단 거리를 찾을 필요가 없음

​	O(N^3^): N 단계에서 N^2^번 연산 수행

​	2차원 리스트 사용 (모든 노드 => 모든 노드 최단 경로)

​	Dynamic Programming



###### 과정

​	단계별로 특정 노드(`C`)를 거쳐 가는 경우를 고려

​	`C`를 제외한 N-1개의 노드 중 서로 다른 2개의 쌍(`A`, `B`)을 선택

​	2개 노드(`A`, `B`)를 `C`를 거쳐 가는 비용을 확인한 후 최단 거리 비용을 갱신 (`A` - `C` - `B`)

​	단계별 ~N-1~P~2~ 번 반복 => O(~N-1~P~2~) ≈ O(N^2^) 이므로

​	N단계를 거치므로 전체 시간복잡도는 O(N^3^)이다.

​	위 과정에 대한 점화식 (K번째 단계)

​	D~ab~ = min(D~ab~, D~ak~ + D~kb~)
​	(= min(cost of A to B, cost of A to K + cost of K to B))



###### 구현

```python
INF = int(1e9)

n = int(input())
e = int(input())

graph = [[INF] * (n+1) for _ in range(n+1)]

# 자기 자신 0으로 초기화
for i in range(1,n+1):
    graph[i][i] = 0
    
for _ in range(e):
    a, b, weight = map(int, input().split())
    graph[a][b] = weight
    
# floyd-warshall
for k in range(1, n+1):
    for a in range(1, n+1):
        for b in range(1, n+1):
            graph[a][b] = min(graph[a][b], graph[a][k] + graph[k][b])
```



---

### Appendix

#### Python에서의 입력 받아오기

##### input()

```python
# 단일 문자
s = input()

# 단일 숫자
n = int(input())

# 여러 숫자 (공백 O)
n, m = map(int,input().split())

# 엔터로 구분되는 n개의 숫자
for _ in range(n):
    l.append(int(input()))
    
# 엔터로 구분되는 n개의 문자, 숫자
for _ in range(n):
    data = input()
    l.append((data[0], int(data[1])))
    
# 공백으로 구분되는 한 줄 내의 모든 숫자
a = list(map(int, input().split()))

# 공백으로 구분되는 한 줄 내의 모든 문자
a = input().split()

# 공백으로 구분되는 숫자 저장 없이 바로 사용 (계수 정렬 등)
for i in range input.split():
    l[int(i)] += 1
```



##### sys.stdin.readline()

​	한 줄 전체 입력 받기

​	입력 데이터가 많을 때 input()을 쓰면 시간 초과가 날 수 있음

​	sys library 사용

```python
import sys

# 문자 한 줄 읽어오기
# 공백 제거용 rstrip()
l = sys.stdin.readline().rstrip()

# 숫자 한 줄 읽어오기
array = list(map(int,sys.stdin.readline().split()))
```



#### 기타 자료 구조

##### Heap

​	Min Heap: 낮은 값 데이터부터 삭제

​	Max Heap: 높은 값 데이터부터 삭제

​	python은 기본적으로 min heap



###### min heap을 max heap처럼 사용하기

​	저장할 때 음수로 만들었다가 꺼낼 때 다시 양수로 바꿔주면 min heap을 max heap처럼 사용 가능



##### Priority Queue

​	PQ는 우선순위가 가장 높은 데이터를 가장 먼저 삭제



###### Python에서의 Priority Queue

​	PriorityQueue, heapq

​	heapq가 동작이 더 빠름

​	heapq는 튜플을 입력받으면 첫 번째 원소를 기준으로 함

